{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 3: Tennis environment\n",
    "Author: Md. Masud Rana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unity ML-Agents [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) Environment\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "This yields a single score for each episode.\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5.\n",
    "\n",
    "<img src=\"images/tennis.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimentation\n",
    "The  maddpg algorithm is an approximate Actor-Critic Method, but also resembles\n",
    "the DQN approach of Reinforcement Learning. The agent is composed of two Neural\n",
    "Networks (NNs  ) the Actor and the Critic, both with target and local networks totalizing 4\n",
    "NNs, these are used to encode the  policy  function.\n",
    "\n",
    "The learning pipeline takes first a state as input in the Actor network, outputting\n",
    "the best possible action in that state, this procedure makes possible for ddpg to tackle\n",
    "continuous action spaces, in contrast to the regular DQN approach. This action is used\n",
    "in the Critic network, alongside with the state, where it outputs an action value function\n",
    "(q), this q is used as a baseline for updating both Actor and Critic networks, reducing\n",
    "the variance and instability of classic RL algorithms. The optimization is done with a\n",
    "gradient ascent between both Actor’s and Critic’s target and local networks parameters.\n",
    "The behaviour of the agent can be explored in the ​ maddpg_agent.py file.\n",
    "\n",
    "Important libraries and components are imported and local parameters are initialized:\n",
    "BUFFER_SIZE,defines the replay buffer size, shared by the agents, this is an object\n",
    "that contains tuples called experiences composed by state,actions,rewards,next states\n",
    "and dones, these are necessary informations for learning; ​ BATCH_SIZE ​ , when the\n",
    "number of experiences in the replay buffer exceeds the batch size, the learning method\n",
    "is called; ​ TAU ​ , this hyperparameter controls the model ​ soft updates , ​ a method used forslowly changing the target networks parameters, improving stability; ​ LR_ACTOR and\n",
    "LR_CRITIC ​ , the optimizer learning rates, these control the gradient ascent step;\n",
    "WEIGHT_DECAY ​ , the l2 regularization parameter of the optimizer.\n",
    "The main implementation begins on fourth step: additional libraries and\n",
    "components are imported, an ​ agent ​ is created and initialized with proper parameters:\n",
    "state_size ​ and ​ action_size. \n",
    "\n",
    "The ​ maddpg function is created, taking as parameters the\n",
    "number of episodes (​ n_episodes) ​ and the maximum length of each episode (​ max_t ​ ).\n",
    "In each episode the environment is reseted and the agents receive initial states.\n",
    "While the number of timesteps is less than ​ max_t, ​ the following procedures are done:\n",
    "The agent use it’s ​ act method with the current state as input, the method takes\n",
    "the input and passes it through the actor network, returning an action for the state. A\n",
    "environment ​ step ​ is taken, using the previous obtained action, and it’s returns: next\n",
    "state, rewards and dones (if the episode is terminated or not). These are stored in the\n",
    "env_info, ​ variable, that passes them individually for each of these information’s new\n",
    "variables. \n",
    "\n",
    "The agent uses it ​ step ​ method, the method first adds the experience tuple for\n",
    "the shared replay buffer and, depending on the size, calls the ​ learn method. The\n",
    "rewards are added to the scores variable and the state receives the next state, to give\n",
    "continuation to the environment, if any of the components of the done variable indicates\n",
    "that the episode is over, the loop of ​ max_t ​ breaks, and a new episode is initialized.\n",
    "If the average score of the last 100 episodes is bigger than 0.5, the networks\n",
    "weights are save and the loop of ​ n_episodes breaks and the ​ maddpg function returns a\n",
    "list with each episode’s score. This list is plotted with the episodes number, showing the\n",
    "Agent’s learning during the algorithm’s execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "There were many hyperparameters involved in the experiment. The value of each of them is given below:\n",
    "\n",
    "|Hyperparameter\t|Value|\n",
    "|:-- | :--: |\n",
    "|Replay buffer size|\t1e5|\n",
    "|Batch size|\t128|\n",
    "|GAMMA(discount factor)\t|0.99|\n",
    "|TAU\t|2e-3|\n",
    "|Actor Learning rate\t|1.5e-3|\n",
    "|Critic Learning rate\t|1.5e-3|\n",
    "|Number of episodes|\t6000|\n",
    "|Max number of timesteps per episode\t|300|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The best performance was achieved by DDPG where the reward of +0.5 was achieved in 1799 episodes. It took so much time to find right parameter. Really it's hard to find right model hyperperameter. \n",
    "\n",
    "<img src=\"images/rewards.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "\n",
    "- Other algorithms like TRPO, PPO, A3C, A2C that have been discussed in the course could potentially lead to better results as well.\n",
    "\n",
    "- The Q-prop algorithm, which combines both off-policy and on-policy learning, could be good one to try.\n",
    "\n",
    "- General optimization techniques like cyclical learning rates and warm restarts could be useful as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
