{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2: Continuous Control\n",
    "#### Author : Md. Masud Rana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project demonstrates how policy-based methods can be used to learn the optimal policy in a model-free Reinforcement Learning setting using a Unity environment, in which a double-jointed arm can move to target locations. \n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Random Agent | Train Agent |\n",
    "| :--: | :--: |\n",
    "|<img src=\"images/random_agent.gif\">|<img src=\"images/reacher.gif\">|\n",
    "|Unity ML-Agents [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) Environment|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic algorithm lying under the hood is an actor-critic method. Policy-based methods like REINFORCE, which use a Monte-Carlo estimate, have the problem of high variance. TD estimates used in value-based methods have low bias and low variance. Actor-critic methods marry these two ideas where the actor is a neural network which updates the policy and the critic is another neural network which evaluates the policy being learned which is, in turn, used to train the actor.\n",
    "\n",
    "[Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/pdf/1509.02971.pdf) lies under the class of Actor Critic Methods but is a bit different than the vanilla Actor-Critic algorithm. The actor produces a deterministic policy instead of the usual stochastic policy and the critic evaluates the deterministic policy. The critic is updated using the TD-error and the actor is trained using the deterministic policy gradient algorithm.\n",
    "\n",
    "\n",
    "<img src=\"images/dpg.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/pdf/1509.02971.pdf) Algorithm\n",
    "\n",
    "<img src=\"images/dpg_algo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "There were many hyperparameters involved in the experiment. The value of each of them is given below:\n",
    "\n",
    "|Hyperparameter\t|Value|\n",
    "|:-- | :--: |\n",
    "|Replay buffer size|\t1e6|\n",
    "|Batch size|\t1024|\n",
    "|GAMMA(discount factor)\t|0.99|\n",
    "|TAU\t|1e-3|\n",
    "|Actor Learning rate\t|1e-4|\n",
    "|Critic Learning rate\t|3e-4|\n",
    "|Update interval|\t20|\n",
    "|Update times per interval\t|10|\n",
    "|Number of episodes|\t500|\n",
    "|Max number of timesteps per episode\t|1000|\n",
    "|Leak for LeakyReLU\t|0.01|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The best performance was achieved by DDPG where the reward of +30 was achieved in 56 episodes. It took so much time to find right parameter. Really it's hard to find right model hyperperameter. \n",
    "\n",
    "<img src=\"images/reward.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "\n",
    "- Other algorithms like TRPO, PPO, A3C, A2C that have been discussed in the course could potentially lead to better results as well.\n",
    "\n",
    "- The Q-prop algorithm, which combines both off-policy and on-policy learning, could be good one to try.\n",
    "\n",
    "- General optimization techniques like cyclical learning rates and warm restarts could be useful as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
