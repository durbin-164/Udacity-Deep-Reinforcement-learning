{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1: Navigation\n",
    "\n",
    "##### Author : Md. Masud Rana\n",
    "\n",
    "- The project is value based method specially 'Deep-Q-Learning' and it's different varialions like 'Double-Deep-Q-Learing', 'Dualing-Deep-Q-Learining' etc. We use Unity Banana Environment for explore that reinforcement learing algorithom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Enviornment Description:\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.  \n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  Given this information, the agent has to learn how to best select actions.  Four discrete actions are available, corresponding to:\n",
    "- **`0`** - move forward.\n",
    "- **`1`** - move backward.\n",
    "- **`2`** - turn left.\n",
    "- **`3`** - turn right.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "At the heart of the learning algorithm is the , Deep Q-learning, which surpassed human-level performance in Atari games. It is an off-policy learning algorithm where the policy being evaluated is different from the policy being learned.\n",
    "\n",
    "To step back for a bit, the idea of Q-learning is to learn the action-value function, often denoted as Q(s, a) , where s represents the current state and a represents the action being evaluated. Q-learning is a form of Temporal-Difference learning (TD-learning), where unlike Monte-Carlo methods, we can learn from each step rather than waiting for an episode to complete. The idea is that once we take an action and are thrust into a new state, we use the current Q-value of that state as the estimate for future rewards.\n",
    "\n",
    "<img src=\"images/q-learning.png\">\n",
    "\n",
    "There's one specific problem here. Since our space is continuous, we can't use a tabular representation. Hence, we use a Function Approximator. The idea behind a function approximator is to introduce a new parameter $\\theta$ that helps us to obtain an approximation of the Q(s, a), $\\hat{Q} (s, a; \\theta)$. So, this becomes a supervised learning problem where the approximation $\\hat{Q}$ represents the expected value and $R + \\gamma * max (Q(s', a))$ becomes the target. We then use mean-square error as the loss function and update the weights accordingly using gradient descent. Now, the choice remains to choose the function approximator. Enter Deep Learning! We use a neural network as function approximator here. More specifically, we choose a 2-hidden layer network with both the layers having 64 hidden units with relu activation applied after each fully-connected layer. Adam was used as the optimizer for finding the optimal weights:\n",
    "\n",
    "<img src=\"images/fa_equation.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
